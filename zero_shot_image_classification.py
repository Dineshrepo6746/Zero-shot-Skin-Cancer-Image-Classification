# -*- coding: utf-8 -*-
"""Zero Shot Image Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RdrJ_fGE2vi5mg3VszE62F1l9TLu8yWp
"""

!pip install torch torchvision transformers datasets pillow

from transformers import pipeline

# Load zero-shot image classification pipeline
classifier = pipeline("zero-shot-image-classification", model="openai/clip-vit-base-patch32")

from transformers import pipeline
from PIL import Image

# Load zero-shot image classification model
classifier = pipeline("zero-shot-image-classification",
                      model="openai/clip-vit-base-patch32",
                      use_fast=True)


# Load the downloaded image
image_path = "/content/cat.png"  # Change this to the correct path
image = Image.open(image_path)

# Define categories
categories = ["cat", "dog", "car", "horse"]

# Perform zero-shot classification
result = classifier(image, candidate_labels=categories)

# Print classification results
for res in result:
    print(f"Label: {res['label']}, Score: {res['score']:.4f}")

!pip install medmnist

!pip install medmnist

import medmnist
from medmnist import INFO
from medmnist.dataset import DermaMNIST  # Correct import

# Load dataset
data = DermaMNIST(split="train", download=True)
print(data)

!pip install torch torchvision transformers datasets medmnist scikit-learn pillow tqdm wandb

import medmnist
from medmnist import DermaMNIST
import torch
import numpy as np

# Download the dataset (like getting a book from the library)
train_dataset = DermaMNIST(split="train", download=True)
test_dataset = DermaMNIST(split="test", download=True)

# Get images and labels (think of them as pictures and their correct names)
train_images = train_dataset.imgs  # NumPy array of images
train_labels = train_dataset.labels.squeeze()  # Labels (actual skin cancer types)

test_images = test_dataset.imgs
test_labels = test_dataset.labels.squeeze()

print(f"Training images: {train_images.shape}, Training labels: {train_labels.shape}")
print(f"Testing images: {test_images.shape}, Testing labels: {test_labels.shape}")

from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image

# Define how we change (transform) the images
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize images to 224x224
    transforms.ToTensor(),  # Convert to tensor (needed for PyTorch)
    transforms.Normalize([0.5], [0.5])  # Normalize (make pixel values between -1 and 1)
])

# Define a custom dataset to apply transformations
class CustomDataset(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = Image.fromarray(self.images[idx])  # Convert NumPy image to PIL image
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, label

# Create training and testing datasets with transformations
train_data = CustomDataset(train_images, train_labels, transform=transform)
test_data = CustomDataset(test_images, test_labels, transform=transform)

# Load data in batches of 32 (like reading a few pages at a time)
train_loader = DataLoader(train_data, batch_size=32, shuffle=True)
test_loader = DataLoader(test_data, batch_size=32, shuffle=False)

from transformers import ViTForImageClassification, ViTFeatureExtractor

# Load OpenAI ViT feature extractor
feature_extractor = ViTFeatureExtractor.from_pretrained("openai/clip-vit-base-patch32")

# Load ViT model, changing output classes to 7 (for skin cancer types)
model = ViTForImageClassification.from_pretrained("openai/clip-vit-base-patch32", num_labels=7)

# Move model to GPU if available (makes training faster)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

import torch.optim as optim
import torch.nn as nn

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()  # Helps measure how wrong the model is
optimizer = optim.Adam(model.parameters(), lr=1e-5)  # Adjusts model weights

# Training function
def train_model(model, train_loader, epochs=5):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        correct = 0
        total = 0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            # Forward pass (prediction)
            outputs = model(images).logits
            loss = criterion(outputs, labels)

            # Backpropagation (learning)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # Compute accuracy
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

        print(f"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}, Accuracy: {100 * correct / total:.2f}%")

# Train the model
train_model(model, train_loader, epochs=5)

def evaluate_model(model, test_loader):
    model.eval()
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images).logits
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

    print(f"Test Accuracy: {100 * correct / total:.2f}%")

# Evaluate model
evaluate_model(model, test_loader)

torch.save(model.state_dict(), "vit_dermamnist.pth")

model.load_state_dict(torch.load("vit_dermamnist.pth"))

def predict_image(model, image_path):
    model.eval()
    image = Image.open(image_path).convert("RGB")
    image = transform(image).unsqueeze(0).to(device)

    with torch.no_grad():
        output = model(image).logits
        _, predicted = torch.max(output, 1)

    print(f"Predicted Class: {predicted.item()}")

# Test
predict_image(model, "/content/Skin.png")

CLASS_NAMES = ["Melanoma", "Nevus", "Basal Cell Carcinoma", "Actinic Keratosis", "Benign Keratosis", "Dermatofibroma", "Vascular Skin Lesion"]

def predict_image(model, image_path):
    model.eval()
    image = Image.open(image_path).convert("RGB")
    image = transform(image).unsqueeze(0).to(device)

    with torch.no_grad():
        output = model(image).logits
        probabilities = torch.nn.functional.softmax(output, dim=1)  # Convert logits to probabilities
        confidence, predicted = torch.max(probabilities, 1)

    print(f"Predicted Class: {CLASS_NAMES[predicted.item()]}, Confidence: {confidence.item():.2%}")

    predict_image(model, "/content/Skin.png")

CLASS_NAMES = [
    "Melanoma",               # Class 0
    "Nevus",                  # Class 1
    "Basal Cell Carcinoma",   # Class 2
    "Actinic Keratosis",      # Class 3
    "Benign Keratosis",       # Class 4
    "Dermatofibroma",         # Class 5
    "Vascular Skin Lesion"    # Class 6
]
def predict_image(model, image_path):
    model.eval()
    image = Image.open(image_path).convert("RGB")
    image = transform(image).unsqueeze(0).to(device)

    with torch.no_grad():
        output = model(image).logits
        probabilities = torch.nn.functional.softmax(output, dim=1)  # Convert logits to probabilities
        confidence, predicted = torch.max(probabilities, 1)

    # Map the predicted index to the class name
    class_name = CLASS_NAMES[predicted.item()]
    print(f"Predicted Class: {class_name}, Confidence: {confidence.item():.2%}")
predict_image(model, "/content/Skin.png")

from torch.utils.data import Dataset, DataLoader
import torch
from torchvision import transforms
from PIL import Image

class SkinCancerDataset(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image = Image.fromarray(self.images[idx])  # Convert from numpy to PIL image
        label = int(self.labels[idx])

        if self.transform:
            image = self.transform(image)

        return image, label

# Define the transformation (resize, tensor, normalize)
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Create the validation DataLoader using the test set
validation_dataset = SkinCancerDataset(test_images, test_labels, transform=transform)
validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)

def evaluate_model(model, validation_loader):
    model.eval()  # Set model to evaluation mode
    correct = 0
    total = 0

    with torch.no_grad():  # Disable gradient calculation for validation
        for images, labels in validation_loader:
            images, labels = images.to(device), labels.to(device)

            # Forward pass
            outputs = model(images).logits
            _, predicted = torch.max(outputs, 1)

            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    accuracy = 100 * correct / total
    return accuracy

validation_accuracy = evaluate_model(model, validation_loader)
print(f'Validation Accuracy (using test set): {validation_accuracy:.2f}%')